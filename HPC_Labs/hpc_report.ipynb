{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-performance computing 2023/24\n",
    "\n",
    "- Name: Sascha Petznick (XXXXXX)\n",
    "- Mail: S.C.Petznick@student.tudelft.nl\n",
    "- Date of Submission: 21.12.2023\n",
    "\n",
    "## Overview\n",
    "\n",
    "- PingPong\n",
    "- MM-Product\n",
    "- Poisson something\n",
    "- Something else\n",
    "\n",
    "• A final report on the lab exercises must be submitted which contains all\n",
    "your answers to the questions in the three exercises.\n",
    "• Any figures, analysis, and tools, illustrating or demonstrating your\n",
    "answers,will be beneficial for your overall grades.\n",
    "• The final report should be completed independently. It should have the\n",
    "following information on the front cover: your name(s), studentnumber(s),\n",
    "email address(s), and submission date.\n",
    "• __Submit your report to Lab reports in Assignments on Brightspace, please also\n",
    "print your reports (no tedious source code) and hand in it to TA T. Deng. After\n",
    "the submission please send us an email. This is to make sure that your report\n",
    "is not unexpectedly got lost.__\n",
    "• Submission Deadline: February 1st 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as pyplot\n",
    "import numpy as np\n",
    "\n",
    "# consider using https://github.com/kirbs-/hide_code to hide specific cells or \n",
    "# use second python script for creating plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PingPong\n",
    "\n",
    "Messages of sizes $2^n$ where $n=0,\\dots,20$ were sent between two MPI nodes. The time needed to send the message to node two, receive it there, send it back to node one, and receive it there was measured. Every message size was sent five times and the time was averaged. Moreover, the test were conducted on the supercomputer using one computing node with two processes and two computing nodes in order to analyze the difference.\n",
    "\n",
    "The results are displayed in plots in Figure ([1](#communication-time-over-message-size)) and ([2](#communication-time-over-message-size)). Based on visual comparison it is clear that using two node has a minor effect on the experiments. Thus, the following analysis will be based on the experiment with one node.\n",
    "\n",
    "Assuming that the message size $t_{\\text{comm}}(m) = \\alpha + \\beta \\cdot m$ is in seconds, we approximate the parameters using a linear regression as $(\\alpha, \\beta) = (6.49\\cdot 10^{-6} [s], 5.713\\cdot 10^{-9} [s/\\text{bytes}])$ as opposed to the exercise sheet. The linear regression was done by dropping the first measurement in order to reduce the effect of the start-up process of MPI. The regression is displayed on Figure (1). It should be mentioned that the plots are displayed in double-logarithmis scaling and the author decided against using two linear regressions and instead used a single one as the logarithmic scaling helps identifying the overall trend and accuracy of the regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Communication time over message size\n",
    "\n",
    "![FigureOfCommTimeOverMessageSizeN1](./figs/pingpong-doublelog_n1.svg)\n",
    "\n",
    "![FigureOfCommTimeOverMessageSizeN2](./figs/pingpong-doublelog_n2.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Based on the visualization, it is clear that the experiment for small message sizes suffers from a higher variance. This may be caused by the overall initialisation overhead that is assumed to play minor role when the message size is increased. Doing so, yields a clear trend of the function towards a linear dependence. One may argue that increasing the message size above magnitudes of $10^4$ bytes requires a second trending. Overall, this may come down to style. A possible reason for the visible jump in times when increasing the message size above $16.384$ bytes is the general size of the buffer of the MPI communicator. It might well be that messages of this size require several messages to be passed instead of fitting into a single buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MM-Product\n",
    "\n",
    "Let $A, B \\in \\mathbb{R}^{n\\times n}$.  \n",
    "\n",
    "### Main idea of distributing the computations\n",
    "\n",
    "The distribution of the matrices is as follows. We distribute the matrix $A$ to all processes using the broadcast operation of MPI. In addition, we scatter rows of $B$ in a blocksize of $\\lfloor n/p\\rfloor$ where $p$ is the number of processes. If it so happens that the rows is not divisible by the number of processes, the last process will work on the rest of the rows that remain when using the blocksize for all the other processes.\n",
    "The following code snippet gives a general idea of the communication that is used ignoring setup and finalisation of the MPI process.\n",
    "\n",
    "```[C]\n",
    "MPI_Bcast(a, n * n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n",
    "MPI_Scatterv(b, counts, displs, MPI_DOUBLE, b_local, counts[rank],\n",
    "    MPI_DOUBLE, 0, MPI_COMM_WORLD);\n",
    "\n",
    "\n",
    "MPI_Gatherv(c_local, counts[rank], MPI_DOUBLE, c, counts, displs,\n",
    "    MPI_DOUBLE, 0, MPI_COMM_WORLD);\n",
    "```\n",
    "\n",
    "In order to optimise the speed of the computations on each node, the matrices $B$ and $C$ are stored in aligned memory in a column-major fashion, which simplifies the calls to `MPI_Scatterv` and `MPI_Gatherv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots\n",
    "\n",
    "For the analysis of the matrix multiplication algorithm, the program was run five times for matrix sizes $10\\times 10$, $100\\times 100$, and $1000\\times 1000$ in a single go where after each iteration allocated memory was freed. \n",
    "Overall, the program was instructed using 1, 2, 8, 24, 48, and 64 computing nodes in order to analyse the effect of an increasing number processes. The timing was started before initialising any matrix including the given matrices $A$ and $B$ of size $n$ as well as the result matrix.\n",
    "\n",
    "Although runs with matrix size of less than 1000 benefitted from increasing the number of computing nodes, we focus on the computations done on matrix size $1000 \\times 1000$. It should be mentioned that increasing the size to even larger matrices would required the program to handle matrices that were rejected by the memory handler of DelftBlue and therefore would require different approaches. Matrices of row size 1000 require about 8MB of memory and fit easily into RAM.\n",
    "\n",
    "Figure ([3](#times-of-run-over-the-number-of-processes)) displays the times needed for each iteration. The time required for the computation appears to reduce by increasing the number of computing nodes. The benefit becomes marginal as the number of processes increases beyond 24 across all sizes of the matrix. However, the average times of the matrix multiplication of sizes 100 using 64 nodes turned out to be slower. This is caused an implementation detail: If the number of nodes is larger than half of the number of rows of the matrices, then the last computing node is required to compute many rows. In this case 37 rows which increases the overall running time significantly. The proposed implementation therefore relies on the assumption that twice as many rows than computing nodes exist in order to distribute the work evenly. This assumption is generally applicable to real-world examples where the size of the matrix is much larger than the number of available computing nodes.   \n",
    "\n",
    "Although not clearly visible in the figure, the first run often took longer that subsequent runs with the same matrix sizes. As the allocated memory of the implemented code is freed after each run we can exclude allocation of memory on the user side from the set of possible explanations if we assume that the operating system has sufficient memory to provide subsequent calls with the same amount of memory. As the program was instructed to require 1GB of RAM and the allocated memory for matrices sums up to around 200MB, this seems to be reasonable. However, the internal buffer size of the MPI operations, in this case Broadcast and Scatter, may require the MPI subprocess to increase the buffer for larger matrices. As the slowdown only happened on the first calls after increasing the matrix sizes this explanation also fits the general experimental setup.\n",
    "Moreover, some runs of matric sizes 10 and 100 took longer than the job of matrix size 1000. For example, for 24 nodes some two runs were slower. Overall, the trend follows the theoretical idea of reducing the computing time when increasing the number of nodes at least if the matrix size is reasonable large enough. Unnecessarily large computing times for small matrices may be circumvented by falling back to an algorithm that operates only on one node.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Times of run over the number of processes\n",
    "\n",
    "![FigureOfTimeOverNumProcesses](./figs/MM-product-time-over-procs.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Parallel solver for the Poisson equation\n",
    "\n",
    "### 1.2.2\n",
    "\n",
    "Running the problem several times with a topology of 4 x 1 yields and optimal value of around 1.93 as suggested by the exercise. For values $\\omega \\in [1.9,1.99]$ we did a binary-like search in order to find this optimal value. The number of iterations using the optimal value turned out to be $131$ taking around $0.042 [s]$.\n",
    "\n",
    "precision goal: 0.000100\n",
    "Number of iterations: 131, omega: 1.93\n",
    "Delta: 0.000066\n",
    "(0) Elapsed Wtime       0.041549 s ( 21.5% CPU)\n",
    "precision goal: 0.000100\n",
    "Delta: 0.000089\n",
    "(1) Elapsed Wtime       0.013035 s ( 82.9% CPU)\n",
    "precision goal: 0.000100\n",
    "Delta: 0.000099\n",
    "(2) Elapsed Wtime       0.077678 s ( 14.0% CPU)\n",
    "precision goal: 0.000100\n",
    "Delta: 0.000078\n",
    "(3) Elapsed Wtime       0.057615 s ( 15.5% CPU)\n",
    "\n",
    "\n",
    "- 4 1 1.93 100x100\n",
    "Number of iterations: 131, omega: 1.93\n",
    "Delta: 0.000066\n",
    "Delta: 0.000089\n",
    "Delta: 0.000099\n",
    "Delta: 0.000078\n",
    "(1) Elapsed Wtime       0.005309 s ( 89.1% CPU)\n",
    "(2) Elapsed Wtime       0.005308 s ( 85.5% CPU)\n",
    "(3) Elapsed Wtime       0.005310 s ( 86.7% CPU)\n",
    "(0) Elapsed Wtime       0.009293 s ( 89.2% CPU)\n",
    "\n",
    "- 2 2 1.93 100x100\n",
    "Number of iterations: 131, omega: 1.93\n",
    "Delta: 0.000089\n",
    "Delta: 0.000082\n",
    "Delta: 0.000099\n",
    "Delta: 0.000087\n",
    "(1) Elapsed Wtime       0.005498 s ( 89.0% CPU)\n",
    "(2) Elapsed Wtime       0.005496 s ( 89.0% CPU)\n",
    "(3) Elapsed Wtime       0.005501 s ( 90.1% CPU)\n",
    "(0) Elapsed Wtime       0.009459 s ( 90.4% CPU)\n",
    "\n",
    "- 4 1 1.93 200x200\n",
    "Number of iterations: 532, omega: 1.93\n",
    "Delta: 0.000063\n",
    "Delta: 0.000098\n",
    "Delta: 0.000099\n",
    "Delta: 0.000078\n",
    "(1) Elapsed Wtime       0.059708 s ( 94.3% CPU)\n",
    "(2) Elapsed Wtime       0.059707 s ( 94.5% CPU)\n",
    "(3) Elapsed Wtime       0.059712 s ( 94.2% CPU)\n",
    "(0) Elapsed Wtime       0.074123 s ( 94.9% CPU)\n",
    "\n",
    "- 2 2 1.93 200x200\n",
    "Number of iterations: 532, omega: 1.93\n",
    "Delta: 0.000098\n",
    "Delta: 0.000096\n",
    "Delta: 0.000099\n",
    "Delta: 0.000098\n",
    "(1) Elapsed Wtime       0.060286 s ( 98.1% CPU)\n",
    "(2) Elapsed Wtime       0.060288 s ( 97.5% CPU)\n",
    "(3) Elapsed Wtime       0.060292 s ( 97.8% CPU)\n",
    "(0) Elapsed Wtime       0.075101 s ( 97.1% CPU)\n",
    "\n",
    "- 4 1 1.93 400x400\n",
    "Number of iterations: 1561, omega: 1.93\n",
    "Delta: 0.000066\n",
    "Delta: 0.000099\n",
    "Delta: 0.000100\n",
    "Delta: 0.000077\n",
    "(1) Elapsed Wtime       0.619254 s ( 98.8% CPU)\n",
    "(2) Elapsed Wtime       0.619255 s ( 99.0% CPU)\n",
    "(3) Elapsed Wtime       0.619252 s ( 98.9% CPU)\n",
    "(0) Elapsed Wtime       0.673155 s ( 99.0% CPU)\n",
    "\n",
    "- 2 2 1.93 400x400\n",
    "Number of iterations: 1561, omega: 1.93\n",
    "Delta: 0.000099\n",
    "Delta: 0.000098\n",
    "Delta: 0.000100\n",
    "Delta: 0.000099\n",
    "(1) Elapsed Wtime       0.620277 s ( 99.3% CPU)\n",
    "(2) Elapsed Wtime       0.620275 s ( 99.1% CPU)\n",
    "(3) Elapsed Wtime       0.620277 s ( 99.0% CPU)\n",
    "(0) Elapsed Wtime       0.674249 s ( 99.3% CPU)\n",
    "\n",
    "- 4 1 1.93 800x800\n",
    "Number of iterations: 3601, omega: 1.93\n",
    "Delta: 0.000071\n",
    "Delta: 0.000100\n",
    "Delta: 0.000100\n",
    "Delta: 0.000072\n",
    "(1) Elapsed Wtime       5.472918 s ( 99.7% CPU)\n",
    "(2) Elapsed Wtime       5.472918 s ( 99.7% CPU)\n",
    "(3) Elapsed Wtime       5.472930 s ( 99.7% CPU)\n",
    "(0) Elapsed Wtime       5.691494 s ( 99.7% CPU)\n",
    "\n",
    "- 2 2 1.93 800x800\n",
    "Number of iterations: 3601, omega: 1.93\n",
    "Delta: 0.000100\n",
    "Delta: 0.000100\n",
    "Delta: 0.000100\n",
    "Delta: 0.000100\n",
    "(1) Elapsed Wtime       5.486153 s ( 99.7% CPU)\n",
    "(2) Elapsed Wtime       5.486152 s ( 99.8% CPU)\n",
    "(3) Elapsed Wtime       5.486152 s ( 99.8% CPU)\n",
    "(0) Elapsed Wtime       5.705497 s ( 99.7% CPU)\n",
    "\n",
    "### 1.2.3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.5\n",
    "\n",
    "The results of the experiments regarding the number of iterations until convergence is reached are presented in the table below.\n",
    "\n",
    "| Topology | $\\omega$ | Grid Size | Number of Iterations |\n",
    "|----------|----------|-----------|-----------------------|\n",
    "| 4 1      | 1.93     | 100x100   | 131                   |\n",
    "| 2 2      | 1.93     | 100x100   | 131                   |\n",
    "| 4 1      | 1.93     | 200x200   | 532                   |\n",
    "| 2 2      | 1.93     | 200x200   | 532                   |\n",
    "| 4 1      | 1.93     | 400x400   | 1561                  |\n",
    "| 2 2      | 1.93     | 400x400   | 1561                  |\n",
    "| 4 1      | 1.93     | 800x800   | 3601                  |\n",
    "| 2 2      | 1.93     | 800x800   | 3601                  |\n",
    "| 4 1      | 1.93     | 1600x1600 | 3981                  |\n",
    "| 2 2      | 1.93     | 1600x1600 | 3981                  |\n",
    "\n",
    "The trend is appears linear until 800x800 as can be seen from the following graphic. However, the number of iterations for 1600x1600 stays below 4000. It seems as if the grid size may be a bad predictor estimating the number of iterations to reach convergence. It should be mentionend that the stopping criterion leads to very different solution for different grid sizes as information propagation is slower on finer grids. \n",
    "\n",
    "The maximum error which is used as a stopping criterion is mainly determined by points close to sources and the error is reduced relatively fast no matter the size of the grid. However, points that a further away from sources will generally attain values of lower magnitude and therefore have no impact on the maximum error that is used.\n",
    "\n",
    "![FigureOfNumIterationsOverGridSize](./figs/exercise01_ex_1_2_5.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.7\n",
    "\n",
    "It was found that there was virtually no increase in performance when restricting the collective communication to every 10th iteration. This appears to relate to the fact that the time spent in `MPI_Allreduce` is less than 0.2% in the case of an 800x800 grid on a topology of 2x2 using $\\omega = 1.93$. The time spent was still reduced to less than 0.1%.\n",
    "\n",
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
